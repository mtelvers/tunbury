---
layout: post
title:  "Q2 Summary"
date:   2025-07-18 12:00:00 +0000
categories: tarides
image:
  path: /images/tarides.png
  thumbnail: /images/thumbs/tarides.png
---

I am grateful for [Tarides](https://tarides.com)' sponsorship of my OCaml work. Below is a summary of my activities in Q2 2025.

# OCaml Infrastructure and Development

## OCaml Maintenance Activities

General maintenance work on OCaml's infrastructure spanned many areas, including [updating minimum supported OCaml versions from 4.02 to 4.08](https://www.tunbury.org/2025/03/24/recent-ocaml-version/) and addressing issues with [opam-repo-ci job timeouts](https://www.tunbury.org/2025/04/04/opam-repo-ci/). Platform-specific work included resolving compatibility issues with [Fedora 42 and GCC 15](https://www.tunbury.org/2025/04/22/ocaml-fedora-gcc/), addressing [Ubuntu AppArmor](https://www.tunbury.org/2025/05/13/ubuntu-apparmor/) conflicts affecting runc operations, and managing [macOS Sequoia](https://www.tunbury.org/2025/05/19/macos-sequoia/) upgrades across the Mac Mini CI workers. Complex build issues were investigated and resolved, including [C++ header path problems in macOS workers](https://www.tunbury.org/2025/06/21/macos-sequoia-include-path/) and [FreeBSD system upgrades](https://www.tunbury.org/2025/03/26/freebsd-14.2/) for the CI infrastructure.

## OCaml Infrastructure Migration

Due to the impending sunset of the [Equinix Metal platform](https://www.tunbury.org/2025/04/23/blade-allocation/), the OCaml community services needed to be migrated. Services including [OCaml-CI](https://www.tunbury.org/2025/04/27/ocaml-ci/), [opam-repo-ci](https://www.tunbury.org/2025/04/29/equinix-moves/), and the [opam.ocaml.org](https://www.tunbury.org/2025/04/29/equinix-moves/) deployment pipeline were migrated to [new blade servers](https://www.tunbury.org/2025/04/25/blade-reallocation/). The migration work was planned to minimise service disruption, which was kept to just a few minutes. Complete procedures were documented, including Docker volume transfers and rsync strategies.

## opam2web Deployment

Optimisation work was undertaken on the [deployment pipeline for opam2web](https://www.tunbury.org/2025/06/24/opam2web/), which powers opam.ocaml.org, to address the more than two-hour deployment time. The primary issue was the enormous size of the opam2web Docker image, which exceeded 25GB due to the inclusion of complete opam package archives. The archive was moved to a separate layer, allowing Docker to cache the layer and reducing the deployment time to 20 minutes.

## opam Dependency Graphs

Algorithms for managing OCaml package dependencies were investigated, including [topological sorting](https://www.tunbury.org/2025/03/25/topological-sort/) to determine the optimal package installation order. This work extended to handling complex dependency scenarios, including post-dependencies and optional dependencies. Implemented a [transitive reduction algorithm](https://www.tunbury.org/2025/06/23/transitive-reduction/) to create a dependency graph with minimal edge counts while preserving the same dependency relationships, enabling more efficient package management and installation processes.

## OCaml Developments under Windows

Significant work was undertaken to bring [containerization](https://www.tunbury.org/2025/06/14/windows-containerd-2/) technologies to OCaml development on Windows. This included implementing a tool to create [host compute networks](https://www.tunbury.org/2025/06/27/windows-containerd-3/) via the Windows API,  tackling limitations with [NTFS hard links](https://www.tunbury.org/2025/06/18/windows-reflinks/), and implementing copy-on-write [reflink](https://www.tunbury.org/2025/07/07/refs-monteverde/) tool for Windows.

## OxCaml Support

Support for the new OxCaml compiler variant included establishing an [opam repository](https://www.tunbury.org/2025/06/12/oxcaml-repository/) and testing which existing [OCaml packages](https://www.tunbury.org/2025/05/14/opam-health-check-oxcaml/) successfully built with the new compiler.

# ZFS Storage and Hardware Deployment

Early in the quarter, a hardware deployment project centred around [Dell PowerEdge R640](https://www.tunbury.org/2025/04/11/dell-r640-ubuntu/) servers with a large-scale SSD storage was undertaken. The project involved deploying multiple batches of [Kingston 7.68TB SSD drives](https://www.tunbury.org/2025/04/03/kingston-drives/), creating automated deployments for Ubuntu using network booting with EFI and cloud-init configuration. Experimented with ZFS implementation as a [root filesystem](https://www.tunbury.org/2025/04/02/ubuntu-with-zfs-root/), which was possibly but ultimately discarded and explored [dm-cache for SSD acceleration](https://www.tunbury.org/2025/04/21/ubuntu-dm-cache/) of spinning disk arrays. Investigated using ZFS as a distributed storage archive system using an [Ansible-based deployment](https://www.tunbury.org/2025/05/16/zfs-replcation-ansible/) strategy based upon a YAML description.

## Talos II Repairs

[Significant hardware reliability issues](https://www.tunbury.org/2025/04/29/raptor-talos-ii/) affected two Raptor Computing Talos II POWER9 machines. The first system experienced complete lockups after as little as 20 minutes of operation, while the second began exhibiting similar problems requiring daily power cycling. Working with Raptor Computing support to isolate the issues, upgrading firmware and eventually [swapping CPUs](https://www.tunbury.org/2025/05/27/raptor-talos-ii-update/) between the systems resolved the issue. Concurrently, this provided an opportunity to analyse the performance of OBuilder operations on POWER9 systems, comparing [OverlayFS on TMPFS versus BTRFS on NVMe storage](https://www.tunbury.org/2025/05/29/overlayfs/), resulting in optimised build performance.

# EEG Systems Investigations

Various software solutions and research platforms were explored as part of a broader system evaluation. This included investigating [Slurm Workload Manager](https://www.tunbury.org/2025/04/14/slurm-workload-manager/) for compute resource scheduling, examining [Gluster distributed filesystem](https://www.tunbury.org/2025/04/19/gluster/) capabilities, and implementing [Otter Wiki with Raven authentication](https://www.tunbury.org/2025/05/07/otter-wiki-with-raven/) integration for collaborative documentation. Research extended to modern research data management platforms, exploring [InvenioRDM](https://www.tunbury.org/2025/06/03/inveniordm/) for scientific data archival and [BON in a Box](https://www.tunbury.org/2025/07/02/bon-in-a-box/) for biodiversity analysis workflows. To support the [Tessera workshop](https://www.tunbury.org/2025/07/14/tessera-workshop/), a multi-user Jupyter environment was set up using Docker containerization.

# Miscellaneous Technical Explorations

Diverse technical explorations included implementing [Bluesky Personal Data Server](https://www.tunbury.org/2025/03/15/bluesky-pds/) and developing innovative [SSH authentication](https://www.tunbury.org/2025/04/25/bluesky-ssh-authentication/) mechanisms using the ATProto network by extracting SSH public keys from Bluesky profiles. Additional projects included developing OCaml-based API tools for [Box cloud storage](https://www.tunbury.org/2025/04/12/box-diff/), creating [Real Time Trains](https://www.tunbury.org/2025/03/23/real-time-trains/) API integrations, and exploring various file synchronisation and [backup](https://www.tunbury.org/2025/06/14/borg-backup/) solutions. Investigation of [reflink copy](https://www.tunbury.org/2025/07/15/reflink-copy/) mechanisms for efficient file operations using OCaml multicore.

